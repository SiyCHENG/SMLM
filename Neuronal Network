import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ConvBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

class DownSampleBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DownSampleBlock, self).__init__()
        self.pool = nn.MaxPool2d(2)
        self.conv = ConvBlock(in_channels, out_channels)

    def forward(self, x):
        x = self.pool(x)  
        x = self.conv(x)  
        return x

class UpSampleBlock(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True):
        super(UpSampleBlock, self).__init__()
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) if bilinear else nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        self.conv = ConvBlock(in_channels, out_channels)

    def forward(self, x, skip=None):
        x = self.up(x)
        if skip is not None:
            x = torch.cat([x, skip], dim=1)
        x = self.conv(x)
        return x

class SingleUNet(nn.Module):
    def __init__(self):
        super(SingleUNet, self).__init__()
        self.entry = ConvBlock(1, 48)  
        self.down1 = DownSampleBlock(48, 96) 
        self.down2 = DownSampleBlock(96, 192)  

        self.up1 = UpSampleBlock(192, 96, bilinear=False)  
        self.up2 = UpSampleBlock(96, 48, bilinear=False) 

    def forward(self, x):
        entry = self.entry(x)
        down1 = self.down1(entry)
        down2 = self.down2(down1)
        up1 = self.up1(down2, down1)
        up2 = self.up2(up1, entry) 
        return up2

class DECODENetwork(nn.Module):
    def __init__(self):
        super(DECODENetwork, self).__init__()
        self.unet1_1 = SingleUNet()
        self.unet1_2 = SingleUNet()
        self.unet1_3 = SingleUNet()

        # feature size after 3 SingleUNet flatten
        feature_size = 230400

        self.combined_features = nn.Linear(feature_size, 256) 

    def forward(self, x1, x2, x3):
        out1 = self.unet1_1(x1)
        out2 = self.unet1_2(x2)
        out3 = self.unet1_3(x3)

        # flat each SingleUNet output
        out1_flat = out1.view(out1.size(0), -1)
        out2_flat = out2.view(out2.size(0), -1)
        out3_flat = out3.view(out3.size(0), -1)

        # combine flatted feature figure
        out_combined_flat = torch.cat([out1_flat, out2_flat, out3_flat], dim=1)

        return out_combined_flat


decode_network = DECODENetwork()

# input matrix x1, x2, x3
x1 = torch.randn(1, 1, 40, 40)
x2 = torch.randn(1, 1, 40, 40)
x3 = torch.randn(1, 1, 40, 40)

output = decode_network(x1, x2, x3)
print(output.shape) 
print(output)
